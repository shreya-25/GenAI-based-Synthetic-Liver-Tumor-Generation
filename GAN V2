"""
IMPROVED Multi-GAN Pipeline for Synthetic Tumor Patch Generation
==================================================================

Key Improvements:
1. Better HU normalization (30-140 HU range based on clinical data)
2. Improved patch extraction with stricter quality filters
3. Enhanced aggregator architecture with attention mechanism
4. Spectral normalization for generator stability
5. Perceptual loss for texture realism
6. Better data augmentation with realistic intensity variations
7. Comprehensive training monitoring

Based on "Label-Free Liver Tumor Segmentation" paper
"""

import os, glob, logging
import numpy as np
import nibabel as nib
from scipy.ndimage import zoom, label as nd_label, center_of_mass
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# ═════════════════════════════════════════════════════════════════════════════
# CONFIGURATION
# ═════════════════════════════════════════════════════════════════════════════
REAL_DATA_DIR = r"C:\Users\sagarwal4\Downloads\LTS_V1\Dataset\trainOriginal_65"
BASE_OUTPUT   = r"C:\Users\sagarwal4\Downloads\LTS_V1\GAN_V2_1127\SyntheticTumorsTest_IMPROVED"
PATCH_SIZE    = (64, 64, 64)
VOXEL_SPACING = (1.0, 1.0, 1.0)

# ⭐ UPDATED: Clinical HU range from paper
# Paper: arterial phase mean = 111 HU (range 32-207), portal venous = 106 HU (range 36-162)
# Tumors typically 11 HU less than liver parenchyma
HU_CLIP_RANGE_TUMOR = (30, 140)  # More realistic range

DEVICE        = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE    = 8
EPOCHS        = 150  # Increased for better convergence
LATENT_DIM    = 64
NUM_SYNTHETIC = 50

os.makedirs(BASE_OUTPUT, exist_ok=True)
CHECKPOINT_DIR = os.path.join(BASE_OUTPUT, "checkpoints")
SYNTHETIC_DIR  = os.path.join(BASE_OUTPUT, "synthetic_tumors")
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs(SYNTHETIC_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)-8s: %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(BASE_OUTPUT, "training.log")),
        logging.StreamHandler()
    ]
)

# ═════════════════════════════════════════════════════════════════════════════
# DATA PREPARATION UTILITIES
# ═════════════════════════════════════════════════════════════════════════════
def load_nifti(path):
    img = nib.load(path)
    return img.get_fdata().astype(np.float32), img.affine, img.header

def save_nifti(data, affine, header, path):
    nib.save(nib.Nifti1Image(data.astype(data.dtype), affine, header), path)

def resample(data, orig_spacing, new_spacing=VOXEL_SPACING, order=1):
    factors = np.array(orig_spacing) / np.array(new_spacing)
    return zoom(data, factors, order=order)

def normalize_hu_tumor(vol, clip=HU_CLIP_RANGE_TUMOR):
    """Normalize HU values specifically for tumor tissue (30-140 HU)"""
    vol = np.clip(vol, *clip)
    return 2 * ((vol - clip[0]) / (clip[1] - clip[0])) - 1

def denormalize_hu_tumor(vol_norm, clip=HU_CLIP_RANGE_TUMOR):
    """Convert normalized values back to tumor-realistic HU range"""
    return ((vol_norm + 1) / 2) * (clip[1] - clip[0]) + clip[0]

def crop_liver_roi(vol, mask):
    coords = np.argwhere(mask > 0)
    if len(coords) == 0:
        return vol, np.array([0, 0, 0])
    mins, maxs = coords.min(axis=0), coords.max(axis=0) + 1
    return vol[mins[0]:maxs[0], mins[1]:maxs[1], mins[2]:maxs[2]], mins

def extract_patch(vol, center, size=PATCH_SIZE):
    start = [int(c - s//2) for c, s in zip(center, size)]
    slices = tuple(slice(max(0, start[i]), max(0, start[i]) + size[i]) for i in range(3))
    patch = np.zeros(size, np.float32)
    region = vol[slices]
    pad_slices = tuple(slice(0, region.shape[i]) for i in range(3))
    patch[pad_slices] = region
    return patch

# ═════════════════════════════════════════════════════════════════════════════
# ⭐ IMPROVED PATCH EXTRACTION WITH QUALITY FILTERING
# ═════════════════════════════════════════════════════════════════════════════
def sample_tumor_patches_improved():
    """Extract high-quality tumor patches with strict filtering"""
    vols = sorted(glob.glob(os.path.join(REAL_DATA_DIR, "volume-*.nii")))
    segs = sorted(glob.glob(os.path.join(REAL_DATA_DIR, "segmentation-*.nii")))
    tumor_patches, tumor_masks = [], []
    
    # ⭐ IMPROVED: Stricter quality filters
    MIN_TUMOR_VOXELS = 1000      # Increased from 500
    MAX_TUMOR_VOXELS = 50000     # Decreased from 100000 (focus on medium tumors)
    MIN_PATCH_TUMOR_CONTENT = 500  # Increased from 300
    MIN_TUMOR_RATIO = 0.15       # At least 15% of patch should be tumor
    MIN_TEXTURE_STD = 0.05       # Avoid blank/uniform patches
    
    logging.info(f"Extracting quality-filtered tumor patches from {len(vols)} volumes...")
    logging.info(f"Using improved HU range: {HU_CLIP_RANGE_TUMOR}")
    logging.info(f"Quality filters: tumor_ratio≥{MIN_TUMOR_RATIO}, texture_std≥{MIN_TEXTURE_STD}")
    
    for vpath, spath in zip(vols, segs):
        logging.info(f"Processing {os.path.basename(vpath)}...")
        vol, _, hdr = load_nifti(vpath)
        seg, _, _ = load_nifti(spath)
        orig_sp = hdr.get_zooms()[:3]
        
        vol_resampled = resample(vol, orig_sp)
        vol_norm = normalize_hu_tumor(vol_resampled)
        seg_resampled = resample(seg, orig_sp, order=0)
        
        vol_roi, offset = crop_liver_roi(vol_norm, seg_resampled)
        seg_roi, _ = crop_liver_roi(seg_resampled, seg_resampled)
        
        if vol_roi.size == 0 or seg_roi.size == 0:
            continue
        
        # Label each individual tumor
        tumor_binary = (seg_roi == 2).astype(np.int32)
        labeled_tumors, num_tumors = nd_label(tumor_binary)
        
        if num_tumors == 0:
            continue
        
        logging.info(f"  Found {num_tumors} individual tumors")
        
        for tumor_id in range(1, num_tumors + 1):
            tumor_mask_3d = (labeled_tumors == tumor_id)
            tumor_size = tumor_mask_3d.sum()
            
            # Skip tumors outside size range
            if tumor_size < MIN_TUMOR_VOXELS or tumor_size > MAX_TUMOR_VOXELS:
                continue
            
            center = center_of_mass(tumor_mask_3d)
            center = np.array(center).astype(int)
            
            patch = extract_patch(vol_roi, center)
            mask = extract_patch(seg_roi, center)
            
            tumor_content = (mask == 2).sum()
            tumor_ratio = tumor_content / np.prod(PATCH_SIZE)
            
            # ⭐ NEW: Enhanced quality checks
            if tumor_content < MIN_PATCH_TUMOR_CONTENT:
                continue
            
            if tumor_ratio < MIN_TUMOR_RATIO:
                continue
            
            # Check texture quality (avoid blank patches)
            tumor_voxels = patch[mask == 2]
            if len(tumor_voxels) > 0 and tumor_voxels.std() < MIN_TEXTURE_STD:
                continue
            
            tumor_patches.append(patch)
            tumor_masks.append((mask == 2).astype(np.float32))
    
    logging.info(f"✅ Extracted {len(tumor_patches)} high-quality tumor patches")
    
    # ⭐ NEW: Report statistics
    if len(tumor_patches) > 0:
        all_hu_values = []
        for patch, mask in zip(tumor_patches, tumor_masks):
            hu_patch = denormalize_hu_tumor(patch)
            tumor_hu = hu_patch[mask > 0]
            all_hu_values.extend(tumor_hu.tolist())
        
        logging.info(f"\nReal Tumor Statistics:")
        logging.info(f"  Mean HU: {np.mean(all_hu_values):.1f}")
        logging.info(f"  Std HU:  {np.std(all_hu_values):.1f}")
        logging.info(f"  Range:   [{np.min(all_hu_values):.1f}, {np.max(all_hu_values):.1f}]")
        logging.info(f"  Median:  {np.median(all_hu_values):.1f}\n")
    
    return tumor_patches, tumor_masks

# ═════════════════════════════════════════════════════════════════════════════
# ⭐ IMPROVED DATA AUGMENTATION
# ═════════════════════════════════════════════════════════════════════════════
class TumorPatchDatasetAugmented(Dataset):
    def __init__(self, patches, masks):
        self.patches = patches
        self.masks = masks
        # Precompute statistics for better augmentation
        self.patch_means = [p.mean() for p in patches]
        self.patch_stds = [p.std() for p in patches]
    
    def __len__(self):
        return len(self.patches) * 5  # 5x augmentation
    
    def __getitem__(self, idx):
        base_idx = idx % len(self.patches)
        patch = self.patches[base_idx].copy()
        mask = self.masks[base_idx].copy()
        
        if idx >= len(self.patches):
            # Random flips (all axes)
            if np.random.rand() > 0.5:
                patch = np.flip(patch, axis=0).copy()
                mask = np.flip(mask, axis=0).copy()
            if np.random.rand() > 0.5:
                patch = np.flip(patch, axis=1).copy()
                mask = np.flip(mask, axis=1).copy()
            if np.random.rand() > 0.5:
                patch = np.flip(patch, axis=2).copy()
                mask = np.flip(mask, axis=2).copy()
            
            # Random rotation (90 degree increments)
            k = np.random.randint(0, 4)
            if k > 0:
                patch = np.rot90(patch, k, axes=(0, 1)).copy()
                mask = np.rot90(mask, k, axes=(0, 1)).copy()
            
            # ⭐ IMPROVED: Better intensity augmentation
            tumor_voxels = patch[mask > 0]
            if len(tumor_voxels) > 0:
                # Intensity shift (simulating contrast variations)
                intensity_shift = np.random.uniform(-0.1, 0.1)
                patch[mask > 0] = patch[mask > 0] + intensity_shift
                
                # Intensity scaling (simulating different tumor densities)
                intensity_scale = np.random.uniform(0.9, 1.1)
                patch[mask > 0] = patch[mask > 0] * intensity_scale
            
            # ⭐ NEW: Add realistic Gaussian noise (tumor texture variation)
            noise_std = self.patch_stds[base_idx] * 0.05
            noise = np.random.randn(*patch.shape) * noise_std
            patch = patch + noise
            
            # Clip to valid range
            patch = np.clip(patch, -1, 1)
        
        return torch.from_numpy(patch)[None].float(), torch.from_numpy(mask)[None].float()

# ═════════════════════════════════════════════════════════════════════════════
# ⭐ IMPROVED GAN ARCHITECTURES
# ═════════════════════════════════════════════════════════════════════════════

class DCGAN3DGenerator(nn.Module):
    """Improved generator with spectral normalization for stability"""
    def __init__(self):
        super().__init__()
        b = 64
        self.net = nn.Sequential(
            # ⭐ NEW: Spectral normalization for training stability
            nn.utils.spectral_norm(nn.ConvTranspose3d(LATENT_DIM, b*8, 4, 1, 0)),
            nn.BatchNorm3d(b*8), nn.ReLU(True),
            nn.utils.spectral_norm(nn.ConvTranspose3d(b*8, b*4, 4, 2, 1)),
            nn.BatchNorm3d(b*4), nn.ReLU(True),
            nn.utils.spectral_norm(nn.ConvTranspose3d(b*4, b*2, 4, 2, 1)),
            nn.BatchNorm3d(b*2), nn.ReLU(True),
            nn.utils.spectral_norm(nn.ConvTranspose3d(b*2, b, 4, 2, 1)),
            nn.BatchNorm3d(b), nn.ReLU(True),
            nn.ConvTranspose3d(b, 1, 4, 2, 1),
            nn.Tanh()
        )
    
    def forward(self, z):
        return self.net(z)

class DCGAN3DDiscriminator(nn.Module):
    def __init__(self):
        super().__init__()
        b = 64
        self.net = nn.Sequential(
            nn.Conv3d(1, b, 4, 2, 1), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b, b*2, 4, 2, 1), nn.BatchNorm3d(b*2), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b*2, b*4, 4, 2, 1), nn.BatchNorm3d(b*4), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b*4, b*8, 4, 2, 1), nn.BatchNorm3d(b*8), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b*8, 1, 4, 1, 0), nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.net(x).view(-1)

class WGAN3DGenerator(DCGAN3DGenerator):
    """Same as DCGAN generator with spectral norm"""
    pass

class WGAN3DCritic(nn.Module):
    def __init__(self):
        super().__init__()
        b = 64
        self.net = nn.Sequential(
            nn.Conv3d(1, b, 4, 2, 1), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b, b*2, 4, 2, 1), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b*2, b*4, 4, 2, 1), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b*4, 1, 4, 1, 0)
        )

    def forward(self, x):
        return self.net(x).view(-1)

class Aggregator3D(nn.Module):
    """⭐ IMPROVED: Better aggregator with attention mechanism and residual connections"""
    def __init__(self):
        super().__init__()
        b = 64  # Increased from 32
        
        # Main processing pathway
        self.conv1 = nn.Conv3d(3, b, 3, 1, 1)
        self.bn1 = nn.BatchNorm3d(b)
        
        self.conv2 = nn.Conv3d(b, b*2, 3, 1, 1)
        self.bn2 = nn.BatchNorm3d(b*2)
        
        self.conv3 = nn.Conv3d(b*2, b, 3, 1, 1)
        self.bn3 = nn.BatchNorm3d(b)
        
        # ⭐ NEW: Attention mechanism to weight the three GAN outputs
        self.attention = nn.Sequential(
            nn.Conv3d(3, 16, 3, 1, 1),
            nn.ReLU(True),
            nn.Conv3d(16, 3, 1, 1, 0),
            nn.Softmax(dim=1)
        )
        
        # Residual pathway
        self.residual = nn.Sequential(
            nn.Conv3d(3, b, 1, 1, 0),
            nn.BatchNorm3d(b)
        )
        
        self.final = nn.Conv3d(b, 1, 3, 1, 1)
        self.act = nn.LeakyReLU(0.2, True)
    
    def forward(self, a, b, c):
        # Stack inputs
        x = torch.cat([a, b, c], 1)
        
        # ⭐ NEW: Learnable attention weights for each GAN
        att_weights = self.attention(x)
        weighted = att_weights[:, 0:1] * a + att_weights[:, 1:2] * b + att_weights[:, 2:3] * c
        
        # Main pathway
        identity = self.residual(x)
        
        out = self.act(self.bn1(self.conv1(x)))
        out = self.act(self.bn2(self.conv2(out)))
        out = self.act(self.bn3(self.conv3(out)))
        
        # ⭐ NEW: Residual connection
        out = out + identity
        
        return torch.tanh(self.final(out))

class StyleTransfer3D(nn.Module):
    """Enhanced style transfer network"""
    def __init__(self):
        super().__init__()
        b = 64  # Increased from 32
        self.net = nn.Sequential(
            nn.Conv3d(1, b, 3, 1, 1), nn.BatchNorm3d(b), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b, b*2, 3, 1, 1), nn.BatchNorm3d(b*2), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b*2, b, 3, 1, 1), nn.BatchNorm3d(b), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b, 1, 3, 1, 1), nn.Tanh()
        )
    
    def forward(self, x):
        return self.net(x)

class Aggregator3DDiscriminator(nn.Module):
    def __init__(self):
        super().__init__()
        b = 64
        self.net = nn.Sequential(
            nn.Conv3d(1, b, 4, 2, 1), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b, b*2, 4, 2, 1), nn.BatchNorm3d(b*2), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b*2, b*4, 4, 2, 1), nn.BatchNorm3d(b*4), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b*4, b*8, 4, 2, 1), nn.BatchNorm3d(b*8), nn.LeakyReLU(0.2, True),
            nn.Conv3d(b*8, 1, 4, 1, 0), nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.net(x).view(-1)

# ═════════════════════════════════════════════════════════════════════════════
# ⭐ NEW: PERCEPTUAL LOSS FOR TEXTURE REALISM
# ═════════════════════════════════════════════════════════════════════════════
class PerceptualLoss3D(nn.Module):
    """Feature matching loss using discriminator features"""
    def __init__(self, discriminator):
        super().__init__()
        self.discriminator = discriminator
        # Build feature extractor by grouping conv blocks properly
        layers = list(discriminator.net.children())
        self.feature_blocks = nn.ModuleList([
            nn.Sequential(layers[0], layers[1]),      # Conv1 + LeakyReLU
            nn.Sequential(layers[2], layers[3], layers[4]),  # Conv2 + BN + LeakyReLU
            nn.Sequential(layers[5], layers[6], layers[7]),  # Conv3 + BN + LeakyReLU
        ])

    def forward(self, fake, real):
        fake_feats = fake
        real_feats = real
        loss = 0

        for block in self.feature_blocks:
            fake_feats = block(fake_feats)
            real_feats = block(real_feats)
            loss += nn.functional.l1_loss(fake_feats, real_feats)

        return loss / len(self.feature_blocks)

def gradient_penalty(critic, real, fake, λ=10):
    if fake.shape != real.shape:
        fake = torch.nn.functional.interpolate(
            fake, size=real.shape[2:],
            mode='trilinear', align_corners=False
        )
    
    α = torch.rand(real.size(0), 1, 1, 1, 1, device=real.device)
    inter = (α * real + (1 - α) * fake).requires_grad_(True)
    out = critic(inter)
    grads = torch.autograd.grad(
        outputs=out, inputs=inter,
        grad_outputs=torch.ones_like(out),
        create_graph=True, retain_graph=True
    )[0]
    return λ * ((grads.view(grads.size(0), -1).norm(2, 1) - 1) ** 2).mean()

# ═════════════════════════════════════════════════════════════════════════════
# ⭐ IMPROVED TRAINING PIPELINE
# ═════════════════════════════════════════════════════════════════════════════
def train_multi_gan(dataloader):
    logging.info("Initializing IMPROVED multi-GAN models...")
    
    # Initialize models
    dc1_g = DCGAN3DGenerator().to(DEVICE)
    dc1_d = DCGAN3DDiscriminator().to(DEVICE)
    dc2_g = DCGAN3DGenerator().to(DEVICE)
    dc2_d = DCGAN3DDiscriminator().to(DEVICE)
    w_g = WGAN3DGenerator().to(DEVICE)
    w_c = WGAN3DCritic().to(DEVICE)
    aggr = Aggregator3D().to(DEVICE)
    style = StyleTransfer3D().to(DEVICE)
    ag_d = Aggregator3DDiscriminator().to(DEVICE)
    
    # ⭐ IMPROVED: Better learning rates (lower for stability)
    dc1_oG = optim.Adam(dc1_g.parameters(), lr=1e-4, betas=(0.5, 0.999))
    dc1_oD = optim.Adam(dc1_d.parameters(), lr=1e-4, betas=(0.5, 0.999))
    dc2_oG = optim.Adam(dc2_g.parameters(), lr=1e-4, betas=(0.5, 0.999))
    dc2_oD = optim.Adam(dc2_d.parameters(), lr=1e-4, betas=(0.5, 0.999))
    w_oG = optim.Adam(w_g.parameters(), lr=2e-5, betas=(0.5, 0.999))
    w_oC = optim.Adam(w_c.parameters(), lr=2e-5, betas=(0.5, 0.999))
    
    # ⭐ NEW: Lower learning rate for aggregator (was struggling before)
    ag_oG = optim.Adam(list(aggr.parameters()) + list(style.parameters()), 
                       lr=5e-5, betas=(0.5, 0.999))
    ag_oD = optim.Adam(ag_d.parameters(), lr=5e-5, betas=(0.5, 0.999))
    
    # ⭐ NEW: Learning rate schedulers
    dc1_sG = optim.lr_scheduler.StepLR(dc1_oG, step_size=40, gamma=0.5)
    dc1_sD = optim.lr_scheduler.StepLR(dc1_oD, step_size=40, gamma=0.5)
    dc2_sG = optim.lr_scheduler.StepLR(dc2_oG, step_size=40, gamma=0.5)
    dc2_sD = optim.lr_scheduler.StepLR(dc2_oD, step_size=40, gamma=0.5)
    ag_sG = optim.lr_scheduler.StepLR(ag_oG, step_size=40, gamma=0.5)
    ag_sD = optim.lr_scheduler.StepLR(ag_oD, step_size=40, gamma=0.5)
    
    # ⭐ NEW: Perceptual loss
    perceptual_loss_fn = PerceptualLoss3D(ag_d).to(DEVICE)
    
    bce = nn.BCELoss()
    history = {
        'dc1_d': [], 'dc1_g': [], 
        'dc2_d': [], 'dc2_g': [], 
        'w_c': [], 'w_g': [], 
        'agg_d': [], 'agg_g': [],
        'perc_loss': []  # Track perceptual loss
    }
    
    logging.info(f"Starting IMPROVED training for {EPOCHS} epochs...")
    logging.info(f"Learning rates: DCGAN={1e-4}, WGAN={2e-5}, Aggregator={5e-5}")
    
    for epoch in range(EPOCHS):
        epoch_losses = {k: [] for k in history.keys()}
        
        for batch_idx, (real, _) in enumerate(dataloader):
            real = real.to(DEVICE)
            bs = real.size(0)
            
            # ════════════════════════════════════════════════════════════
            # Train DCGAN 1
            # ════════════════════════════════════════════════════════════
            dc1_oD.zero_grad()
            z1 = torch.randn(bs, LATENT_DIM, 1, 1, 1, device=DEVICE)
            fake1 = dc1_g(z1)
            d_real = dc1_d(real)
            d_fake = dc1_d(fake1.detach())
            d_loss = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))
            d_loss.backward()
            dc1_oD.step()
            
            dc1_oG.zero_grad()
            d_fake = dc1_d(fake1)
            g_loss = bce(d_fake, torch.ones_like(d_fake))
            g_loss.backward()
            # ⭐ NEW: Gradient clipping
            torch.nn.utils.clip_grad_norm_(dc1_g.parameters(), max_norm=1.0)
            dc1_oG.step()
            epoch_losses['dc1_d'].append(d_loss.item())
            epoch_losses['dc1_g'].append(g_loss.item())
            
            # ════════════════════════════════════════════════════════════
            # Train DCGAN 2
            # ════════════════════════════════════════════════════════════
            dc2_oD.zero_grad()
            z2 = torch.randn(bs, LATENT_DIM, 1, 1, 1, device=DEVICE)
            fake2 = dc2_g(z2)
            d_real2 = dc2_d(real)
            d_fake2 = dc2_d(fake2.detach())
            d_loss2 = bce(d_real2, torch.ones_like(d_real2)) + bce(d_fake2, torch.zeros_like(d_fake2))
            d_loss2.backward()
            dc2_oD.step()
            
            dc2_oG.zero_grad()
            d_fake2 = dc2_d(fake2)
            g_loss2 = bce(d_fake2, torch.ones_like(d_fake2))
            g_loss2.backward()
            torch.nn.utils.clip_grad_norm_(dc2_g.parameters(), max_norm=1.0)
            dc2_oG.step()
            epoch_losses['dc2_d'].append(d_loss2.item())
            epoch_losses['dc2_g'].append(g_loss2.item())
            
            # ════════════════════════════════════════════════════════════
            # Train WGAN
            # ════════════════════════════════════════════════════════════
            for _ in range(3):
                w_oC.zero_grad()
                z3 = torch.randn(bs, LATENT_DIM, 1, 1, 1, device=DEVICE)
                fake3 = w_g(z3)
                c_real = w_c(real)
                c_fake = w_c(fake3.detach())
                gp = gradient_penalty(w_c, real, fake3)
                c_loss = -(c_real.mean() - c_fake.mean()) + gp
                c_loss.backward()
                w_oC.step()
            
            w_oG.zero_grad()
            z3 = torch.randn(bs, LATENT_DIM, 1, 1, 1, device=DEVICE)
            fake3 = w_g(z3)
            g_loss3 = -w_c(fake3).mean()
            g_loss3.backward()
            w_oG.step()
            epoch_losses['w_c'].append(c_loss.item())
            epoch_losses['w_g'].append(g_loss3.item())
            
            # ════════════════════════════════════════════════════════════
            # Train Aggregator with Perceptual Loss
            # ════════════════════════════════════════════════════════════
            ag_oD.zero_grad()
            with torch.no_grad():
                z1 = torch.randn(bs, LATENT_DIM, 1, 1, 1, device=DEVICE)
                z2 = torch.randn(bs, LATENT_DIM, 1, 1, 1, device=DEVICE)
                z3 = torch.randn(bs, LATENT_DIM, 1, 1, 1, device=DEVICE)
                agg_fake = aggr(dc1_g(z1), dc2_g(z2), w_g(z3))
                styled = style(agg_fake)
            
            ag_d_real = ag_d(real)
            ag_d_fake = ag_d(styled.detach())
            ag_d_loss = bce(ag_d_real, torch.ones_like(ag_d_real)) + bce(ag_d_fake, torch.zeros_like(ag_d_fake))
            ag_d_loss.backward()
            ag_oD.step()
            
            # ⭐ IMPROVED: Aggregator generator with perceptual loss
            ag_oG.zero_grad()
            z1 = torch.randn(bs, LATENT_DIM, 1, 1, 1, device=DEVICE)
            z2 = torch.randn(bs, LATENT_DIM, 1, 1, 1, device=DEVICE)
            z3 = torch.randn(bs, LATENT_DIM, 1, 1, 1, device=DEVICE)
            agg_fake = aggr(dc1_g(z1), dc2_g(z2), w_g(z3))
            styled = style(agg_fake)
            ag_g_out = ag_d(styled)
            
            # Adversarial loss
            ag_g_loss = bce(ag_g_out, torch.ones_like(ag_g_out))
            
            # ⭐ NEW: Add perceptual loss for texture realism
            perc_loss = perceptual_loss_fn(styled, real) * 0.1  # Weight factor
            
            total_g_loss = ag_g_loss + perc_loss
            total_g_loss.backward()
            torch.nn.utils.clip_grad_norm_(list(aggr.parameters()) + list(style.parameters()), max_norm=1.0)
            ag_oG.step()
            
            epoch_losses['agg_d'].append(ag_d_loss.item())
            epoch_losses['agg_g'].append(ag_g_loss.item())
            epoch_losses['perc_loss'].append(perc_loss.item())
            
            # ════════════════════════════════════════════════════════════
            # Logging
            # ════════════════════════════════════════════════════════════
            if batch_idx % 10 == 0:
                logging.info(
                    f"Epoch [{epoch+1}/{EPOCHS}] Batch [{batch_idx}/{len(dataloader)}] | "
                    f"DC1_D: {d_loss:.4f}, DC1_G: {g_loss:.4f} | "
                    f"DC2_D: {d_loss2:.4f}, DC2_G: {g_loss2:.4f} | "
                    f"W_C: {c_loss:.4f}, W_G: {g_loss3:.4f} | "
                    f"Agg_D: {ag_d_loss:.4f}, Agg_G: {ag_g_loss:.4f}, Perc: {perc_loss:.4f}"
                )
        
        # Update history
        for key in history.keys():
            history[key].append(np.mean(epoch_losses[key]))
        
        # ⭐ IMPROVED: Monitor HU distribution every 10 epochs
        if (epoch + 1) % 10 == 0:
            with torch.no_grad():
                dc1_g.eval(); dc2_g.eval(); w_g.eval(); aggr.eval(); style.eval()
                
                # Generate multiple samples
                z_samples = [torch.randn(5, LATENT_DIM, 1, 1, 1, device=DEVICE) for _ in range(3)]
                styled_samples = style(aggr(dc1_g(z_samples[0]), 
                                           dc2_g(z_samples[1]), 
                                           w_g(z_samples[2])))
                
                styled_hu = denormalize_hu_tumor(styled_samples.cpu().numpy())
                
                # Compare with real patches
                real_sample = next(iter(dataloader))[0][:5].to(DEVICE)
                real_hu = denormalize_hu_tumor(real_sample.cpu().numpy())
                
                logging.info(f"\n{'='*70}")
                logging.info(f"Epoch {epoch+1} - HU STATISTICS COMPARISON:")
                logging.info(f"  GENERATED - Mean: {styled_hu.mean():.1f}, Std: {styled_hu.std():.1f}, "
                           f"Range: [{styled_hu.min():.1f}, {styled_hu.max():.1f}]")
                logging.info(f"  REAL      - Mean: {real_hu.mean():.1f}, Std: {real_hu.std():.1f}, "
                           f"Range: [{real_hu.min():.1f}, {real_hu.max():.1f}]")
                logging.info(f"  TARGET    - Range: {HU_CLIP_RANGE_TUMOR}, Typical mean: 95-110 HU")
                logging.info(f"{'='*70}\n")
                
                dc1_g.train(); dc2_g.train(); w_g.train(); aggr.train(); style.train()
        
        # ⭐ NEW: Step learning rate schedulers
        if (epoch + 1) % 40 == 0:
            dc1_sG.step()
            dc1_sD.step()
            dc2_sG.step()
            dc2_sD.step()
            ag_sG.step()
            ag_sD.step()
            logging.info(f"Learning rate decreased at epoch {epoch+1}")
    
    # Save final models
    logging.info("Saving final model checkpoints...")
    models = {"dc1_g": dc1_g, "dc1_d": dc1_d, "dc2_g": dc2_g, "dc2_d": dc2_d,
              "w_g": w_g, "w_c": w_c, "aggr": aggr, "style": style, "ag_d": ag_d}
    for name, model in models.items():
        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f"{name}_final.pth"))
    
    return dc1_g, dc2_g, w_g, aggr, style, history

# ═════════════════════════════════════════════════════════════════════════════
# GENERATE SYNTHETIC TUMORS
# ═════════════════════════════════════════════════════════════════════════════
def generate_synthetic_tumors_fixed(models, num_samples=50):
    dc1_g, dc2_g, w_g, aggr, style = models
    for m in models:
        m.eval()
    
    vols = sorted(glob.glob(os.path.join(REAL_DATA_DIR, "volume-*.nii")))
    sample_img = nib.load(vols[0])
    aff, hdr = sample_img.affine, sample_img.header
    
    logging.info(f"Generating {num_samples} synthetic tumor samples...")
    hu_values_all = []
    
    for i in range(num_samples):
        with torch.no_grad():
            z = lambda: torch.randn(1, LATENT_DIM, 1, 1, 1, device=DEVICE)
            aggregated = aggr(dc1_g(z()), dc2_g(z()), w_g(z()))
            styled = style(aggregated)
            
            tumor_volume = styled.squeeze().cpu().numpy()
            tumor_mask = (tumor_volume > 0).astype(np.int16)
            tumor_hu = denormalize_hu_tumor(tumor_volume, HU_CLIP_RANGE_TUMOR)
            
            tumor_voxels = tumor_hu[tumor_mask > 0]
            if len(tumor_voxels) > 0:
                hu_values_all.extend(tumor_voxels.tolist())
        
        save_nifti(tumor_hu, aff, hdr, os.path.join(SYNTHETIC_DIR, f"synthetic_tumor_{i:03d}.nii"))
        save_nifti(tumor_mask, aff, hdr, os.path.join(SYNTHETIC_DIR, f"synthetic_mask_{i:03d}.nii"))
        
        if (i + 1) % 10 == 0:
            logging.info(f"Generated {i+1}/{num_samples} samples")
    
    # Final statistics
    if len(hu_values_all) > 0:
        logging.info(f"\n{'='*70}")
        logging.info(f"FINAL GENERATED TUMOR HU STATISTICS:")
        logging.info(f"  Mean:   {np.mean(hu_values_all):.1f} HU")
        logging.info(f"  Median: {np.median(hu_values_all):.1f} HU")
        logging.info(f"  Std:    {np.std(hu_values_all):.1f} HU")
        logging.info(f"  Range:  [{np.min(hu_values_all):.1f}, {np.max(hu_values_all):.1f}] HU")
        logging.info(f"  Target: {HU_CLIP_RANGE_TUMOR} HU (clinical range)")
        logging.info(f"{'='*70}\n")
    
    logging.info(f"✅ Saved {num_samples} synthetic tumors to {SYNTHETIC_DIR}")

# ═════════════════════════════════════════════════════════════════════════════
# VISUALIZATION
# ═════════════════════════════════════════════════════════════════════════════
def plot_training_results(history):
    epochs = range(1, len(history['dc1_d']) + 1)
    fig, axes = plt.subplots(2, 4, figsize=(24, 10))
    fig.suptitle('Multi-GAN Training Results (IMPROVED)', fontsize=16, fontweight='bold')
    
    # DCGAN 1
    axes[0, 0].plot(epochs, history['dc1_d'], 'b-', label='Discriminator', linewidth=2)
    axes[0, 0].plot(epochs, history['dc1_g'], 'r-', label='Generator', linewidth=2)
    axes[0, 0].set_title('DCGAN 1', fontweight='bold')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    
    # DCGAN 2
    axes[0, 1].plot(epochs, history['dc2_d'], 'b-', label='Discriminator', linewidth=2)
    axes[0, 1].plot(epochs, history['dc2_g'], 'r-', label='Generator', linewidth=2)
    axes[0, 1].set_title('DCGAN 2', fontweight='bold')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    
    # WGAN
    axes[0, 2].plot(epochs, history['w_c'], 'b-', label='Critic', linewidth=2)
    axes[0, 2].plot(epochs, history['w_g'], 'r-', label='Generator', linewidth=2)
    axes[0, 2].set_title('WGAN', fontweight='bold')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Loss')
    
    # Aggregator
    axes[0, 3].plot(epochs, history['agg_d'], 'b-', label='Discriminator', linewidth=2)
    axes[0, 3].plot(epochs, history['agg_g'], 'r-', label='Generator', linewidth=2)
    axes[0, 3].plot(epochs, history['perc_loss'], 'g-', label='Perceptual', linewidth=2)
    axes[0, 3].set_title('Aggregator', fontweight='bold')
    axes[0, 3].legend()
    axes[0, 3].grid(True, alpha=0.3)
    axes[0, 3].set_xlabel('Epoch')
    axes[0, 3].set_ylabel('Loss')
    
    # All Discriminators
    axes[1, 0].plot(epochs, history['dc1_d'], label='DCGAN1 D', linewidth=2)
    axes[1, 0].plot(epochs, history['dc2_d'], label='DCGAN2 D', linewidth=2)
    axes[1, 0].plot(epochs, history['agg_d'], label='Agg D', linewidth=2)
    axes[1, 0].set_title('All Discriminators', fontweight='bold')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Loss')
    
    # All Generators
    axes[1, 1].plot(epochs, history['dc1_g'], label='DCGAN1 G', linewidth=2)
    axes[1, 1].plot(epochs, history['dc2_g'], label='DCGAN2 G', linewidth=2)
    axes[1, 1].plot(epochs, history['w_g'], label='WGAN G', linewidth=2)
    axes[1, 1].plot(epochs, history['agg_g'], label='Agg G', linewidth=2)
    axes[1, 1].set_title('All Generators', fontweight='bold')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Loss')
    
    # Perceptual Loss Detail
    axes[1, 2].plot(epochs, history['perc_loss'], 'g-', linewidth=2)
    axes[1, 2].set_title('Perceptual Loss (Texture Realism)', fontweight='bold')
    axes[1, 2].grid(True, alpha=0.3)
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Loss')
    
    # Generator comparison (stability check)
    axes[1, 3].plot(epochs, history['dc1_g'], label='DCGAN1', linewidth=2, alpha=0.7)
    axes[1, 3].plot(epochs, history['dc2_g'], label='DCGAN2', linewidth=2, alpha=0.7)
    axes[1, 3].axhline(y=np.mean(history['dc1_g'][-20:]), color='b', linestyle='--', label='DC1 final avg')
    axes[1, 3].axhline(y=np.mean(history['dc2_g'][-20:]), color='r', linestyle='--', label='DC2 final avg')
    axes[1, 3].set_title('Generator Stability Check', fontweight='bold')
    axes[1, 3].legend()
    axes[1, 3].grid(True, alpha=0.3)
    axes[1, 3].set_xlabel('Epoch')
    axes[1, 3].set_ylabel('Loss')
    
    plt.tight_layout()
    plt.savefig(os.path.join(BASE_OUTPUT, "training_loss_curves_improved.png"), dpi=150, bbox_inches='tight')
    logging.info(f"Saved training curves to {BASE_OUTPUT}/training_loss_curves_improved.png")
    plt.close()

# ═════════════════════════════════════════════════════════════════════════════
# MAIN EXECUTION
# ═════════════════════════════════════════════════════════════════════════════
if __name__ == "__main__":
    logging.info("\n" + "="*80)
    logging.info("IMPROVED SYNTHETIC TUMOR GENERATION PIPELINE")
    logging.info("="*80)
    logging.info("Key Improvements:")
    logging.info("  ✓ Updated HU range: " + str(HU_CLIP_RANGE_TUMOR) + " (clinically accurate)")
    logging.info("  ✓ Spectral normalization in generators")
    logging.info("  ✓ Enhanced aggregator with attention mechanism")
    logging.info("  ✓ Perceptual loss for texture realism")
    logging.info("  ✓ Better data augmentation with intensity variations")
    logging.info("  ✓ Stricter patch quality filtering")
    logging.info("  ✓ Learning rate scheduling")
    logging.info("  ✓ Gradient clipping for stability")
    logging.info(f"  ✓ Training for {EPOCHS} epochs")
    logging.info("="*80 + "\n")
    
    # Extract patches
    tumor_patches, tumor_masks = sample_tumor_patches_improved()
    
    if len(tumor_patches) == 0:
        logging.error("❌ No tumor patches extracted! Check your data.")
        exit(1)
    
    # Create dataset
    dataset = TumorPatchDatasetAugmented(tumor_patches, tumor_masks)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    logging.info(f"Dataset: {len(tumor_patches)} base patches → {len(dataset)} with 5x augmentation\n")
    
    # Train
    *trained_models, history = train_multi_gan(dataloader)
    
    # Plot results
    plot_training_results(history)
    
    # Generate synthetic tumors
    generate_synthetic_tumors_fixed(trained_models, num_samples=NUM_SYNTHETIC)
    
    logging.info("\n" + "="*80)
    logging.info("✅ IMPROVED PIPELINE COMPLETE!")
    logging.info(f"Output directory: {BASE_OUTPUT}")
    logging.info("\nExpected Improvements:")
    logging.info("  • Generator loss should be more stable (decreasing/flat)")
    logging.info("  • Aggregator should blend GANs better")
    logging.info("  • Generated HU values should match real tumors (30-140 range)")
    logging.info("  • Textures should be more realistic due to perceptual loss")
    logging.info("="*80 + "\n")
